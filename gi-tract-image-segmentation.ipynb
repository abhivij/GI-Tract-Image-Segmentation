{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":27923,"databundleVersionId":3495119,"sourceType":"competition"},{"sourceId":548601,"sourceType":"modelInstanceVersion","modelInstanceId":419272,"modelId":436930}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install & Import","metadata":{}},{"cell_type":"code","source":"import socket\ndef internet_on(host=\"8.8.8.8\", port=53, timeout=3):\n    '''\n    Host: 8.8.8.8 (Google DNS)\n    Open socket to test connectivity\n    '''\n    try:\n        socket.setdefaulttimeout(timeout)\n        socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((host, port))\n        return True\n    except Exception:\n        return False\n\nif internet_on():\n    !pip install segmentation_models_pytorch\n    !pip install monai\nelse:\n    print('Internet off - Relying on dependency installation code')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T11:26:40.683112Z","iopub.execute_input":"2025-08-29T11:26:40.683376Z","execution_failed":"2025-08-29T11:27:31.566Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7aff3e32a8d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/segmentation-models-pytorch/\u001b[0m\u001b[33m\n\u001b[0m^C\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport random\nimport re\n\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\n\nfrom glob import glob\n\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nfrom sklearn.model_selection import StratifiedGroupKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import ToTensor\n\nimport albumentations as A\n\nimport segmentation_models_pytorch as smp\n\nfrom monai.metrics.utils import get_mask_edges, get_surface_distance","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torchmetrics\n# print(torchmetrics.__version__)\n# 1.7.1","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Number of available CPUs: {os.cpu_count()}\")\nprint(f\"Number of available GPUs: {torch.cuda.device_count()}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.569Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Global variables","metadata":{}},{"cell_type":"code","source":"DIR_PATH = \"/kaggle/input/uw-madison-gi-tract-image-segmentation/\"\n\npd.set_option('display.max_colwidth', 400) \n\nCMAP1 = ListedColormap([[0, 0, 0, 0], [1, 0, 0, 1]])  # black transparent, red opaque\nCMAP2 = ListedColormap([[0, 0, 0, 0], [0, 1, 0, 1]])  # black transparent, green opaque\nCMAP3 = ListedColormap([[0, 0, 0, 0], [0, 0, 1, 1]])  # black transparent, blue opaque\n\nRANDOM_SEED = 0\n\nIMAGE_NORMALIZE_MEAN = (0.485, 0.456, 0.406)\nIMAGE_NORMALIZE_SD = (0.229, 0.224, 0.225)\n\nIMAGE_RESIZE = [288, 288]\n\nBATCH_SIZE_TRAIN = 32\nBATCH_SIZE_VALID = BATCH_SIZE_TRAIN*2\nBATCH_SIZE_TEST = BATCH_SIZE_TRAIN*2\n\nDATA_LOADER_NUM_WORKERS = 4\n\nNUM_CLASSES = 3\nCLASS_NAMES = ['large_bowel', 'small_bowel', 'stomach']\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nEPOCHS = 5\n\nMODEL_PARAMS_FILE_NAME = \"GIT-Seg-efficientnet-b1-subset.pth\"\nMODEL_PARAMS_LOAD_FILE_PATH = \"/kaggle/input/git-seg/pytorch/default/1/GIT-Seg-efficientnet-b1-subset.pth\"\n\nTRAIN_VALID_SPLIT = False\n# TRAIN_ON_FULL_DATA = True\nTEST_PREDICT = True\n\nSAVE_TRAIN_VALID_MODEL = False\nLOAD_MODEL_FOR_TEST_PREDICT = True","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ensure reproducibility(to some extent) across different runs\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing & EDA","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(DIR_PATH + \"train.csv\")\ndata.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_nonnaseg = data.loc[data.segmentation.notna(), :]\ndata_nonnaseg.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[['case', 'day', 'slice']] = data['id'].str.extract(r'case(\\d+)_day(\\d+)_slice_(\\d+)')\ndata","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The image file corresponding to case123_day20_slice_0065 is train/case123/case123_day20/scans/slice_0065_266_266_1.50_1.50.png \n# 266, 266 are slice width, slice height and 1.5, 1.5 are pixel width, pixel height.\n\ndef get_path_df(train = True):\n    if train:\n        paths = glob(DIR_PATH + 'train/*/*/*/*')\n    else:\n        paths = glob(DIR_PATH + 'test/*/*/*/*')\n    path_df = pd.DataFrame(paths, columns=['image_path'])\n    path_df[['case', 'day', 'slice', \n             'slice_w', 'slice_h', \n             'px_w', 'px_h']] = \\\n            path_df.image_path.str.extract(r'.*/case(\\d+)_day(\\d+)/scans/slice_(\\d+)_(\\d+)_(\\d+)_(\\d+\\.\\d+)_(\\d+\\.\\d+)\\.png')\n    \n    return path_df\n\npath_df = get_path_df()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path_df.info()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see that the number of rows in data df is 3x that of path_df. \nEach (case, day, slice) entry in path_df has 3 matching entries in data, corresponding to the 3 segmentation classes","metadata":{}},{"cell_type":"code","source":"data = data.merge(path_df, on = ['case', 'day', 'slice'])\ndata","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.px_w.unique(), data.px_h.unique()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.case.unique(), data.day.unique(), data.slice.unique(), data.slice_w.unique(), data.slice_h.unique()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"int_cols = ['case', 'day', 'slice', 'slice_w', 'slice_h']\ndata[int_cols] = data[int_cols].astype(np.uint32)\n\nfloat_cols = ['px_w', 'px_h']\ndata[float_cols] = data[float_cols].astype(np.float32)\n\ndata.info()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.577Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run Length Encoding (RLE)","metadata":{}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formatted (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = np.asarray(mask_rle.split(), dtype=int)\n    starts = s[0::2] - 1\n    lengths = s[1::2]\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction\n\n\n# ref: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.577Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## get_mask","metadata":{}},{"cell_type":"code","source":"def get_mask(id_, data):\n    data_subset_id = data.loc[data['id']==id_]\n    slice_dim = data_subset_id[['slice_h', 'slice_w']].iloc[0]\n    shape = (slice_dim.slice_h, slice_dim.slice_w, 3)\n    mask = np.zeros(shape, dtype=np.uint8)\n    for i, class_ in enumerate(CLASS_NAMES):\n        data_subset_class = data_subset_id[data_subset_id['class']==class_]\n        rle = data_subset_class.segmentation.squeeze()\n        if not pd.isna(rle):\n            mask[..., i] = rle_decode(rle, shape[:2])\n    return mask","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_image_file_path = DIR_PATH + \"train/case123/case123_day20/scans/slice_0065_266_266_1.50_1.50.png\"\n\nimg = cv2.imread(full_image_file_path, cv2.IMREAD_UNCHANGED)\n# default imread mode is IMREAD_COLOR which expects 8-bit 3 channel image, our input image is 16-bit grayscale which requires IMREAD_UNCHANGED\n\nprint(img.shape)\nprint(img)\n\nplt.figure(figsize=(8, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(img, cmap='gray')\nplt.title('Gray')\nplt.axis('off')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.imshow(img, cmap='bone')\nplt.title('Bone')\nplt.axis('off')\nplt.colorbar()\n\n# while gray cmap is technically most correct for 16-bit grayscale image, using bone cmap from now on to enhance contrast visually\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = cv2.imread(full_image_file_path, cv2.IMREAD_UNCHANGED).astype('float32')\n\nimg_norm = img\nmx = np.max(img)\nif mx > 0:\n    img_norm /= mx\n\nprint(img_norm)\nprint(img_norm.shape)\nprint(max([max(r) for r in img_norm]))\n\nimg_norm = (img_norm*255).astype(np.uint8)\nprint(img_norm)\nprint(img_norm.shape)\nprint(max([max(r) for r in img_norm]))\n\nclahe1 = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\nclahe2 = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2,2))\nclahe3 = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(2,2))\n\nres1 = clahe1.apply(img_norm)\nres2 = clahe2.apply(img_norm)\nres3 = clahe3.apply(img_norm)\n\nprint(res1)\nprint(res1.shape)\nprint(max([max(r) for r in res1]))\n\nprint(res2)\nprint(res2.shape)\nprint(max([max(r) for r in res2]))\n\n# Show results\nplt.figure(figsize=(20, 4))\nfor i, (title, im) in enumerate(zip(['Original', 'Normalized', 'CLAHE clip=2 grid=8x8', 'CLAHE clip=2 grid=2x2', 'CLAHE clip=1 grid=2x2'], [img, img_norm, res1, res2, res3])):\n    plt.subplot(1,5,i+1)\n    plt.imshow(im, cmap='bone')\n    plt.title(title)\n    plt.colorbar()\n    plt.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Based on the above figures, decided to use CLAHE clip=1 grid=2x2 for best visualization","metadata":{}},{"cell_type":"markdown","source":"## display_image","metadata":{}},{"cell_type":"code","source":"def load_image(id_, data):\n    data_subset = data.loc[data.id == id_]\n    img = cv2.imread(data_subset.image_path.iloc[0], cv2.IMREAD_UNCHANGED).astype('float32')  # convert from original 16-bit\n    #print(f'Raw image {id_} min : {np.min(img)} max : {np.max(img)}')\n    mx = np.max(img)\n    if mx > 0:\n        img /= mx\n    return img","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_image(id_, data, pred_mask=None, apply_CLAHE=False,\n                  show_orig_img=True, show_true_mask=True, show_pred_mask=False):\n    \n    img = load_image(id_, data)\n    #print(f'load_image result {id_} min : {np.min(img)} max : {np.max(img)}')\n    img = (img * 255).astype(np.uint8) # 0-255 range required for CLAHE. \n                                       # Using this in general to maintain consistency with the case where CLAHE is required\n    #print(f'just before CLAHE {id_} min : {np.min(img)} max : {np.max(img)}')\n    if apply_CLAHE:\n        clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(2,2))\n        img = clahe.apply(img)\n    \n    mask = get_mask(id_, data)\n\n    plt.figure(figsize=(9, 3))\n    \n    i = 1\n    if show_orig_img:\n        plt.subplot(1, 3, i)\n        i += 1\n        #print(f'just before imshow {id_} min : {np.min(img)} max : {np.max(img)}')\n        plt.imshow(img, cmap='bone')\n        plt.title(f'{id_} image')\n        plt.axis('off')\n\n    if show_true_mask:\n        plt.subplot(1, 3, i)\n        i += 1\n        print(f'just before imshow {id_} min : {np.min(img)} max : {np.max(img)}')\n        print(img.shape)\n        plt.imshow(img, cmap='bone')\n        plt.title('Image with true mask')\n        plt.imshow(mask[..., 0], cmap=CMAP1)\n        plt.imshow(mask[..., 1], cmap=CMAP2)\n        plt.imshow(mask[..., 2], cmap=CMAP3)\n        \n        handles = [\n            Rectangle((0, 0), 1, 1, color=CMAP1(1.0)),\n            Rectangle((0, 0), 1, 1, color=CMAP2(1.0)),\n            Rectangle((0, 0), 1, 1, color=CMAP3(1.0))\n        ]\n        labels = ['Large Bowel', 'Small Bowel', 'Stomach']\n        plt.axis('off')\n        plt.legend(handles, labels, bbox_to_anchor=(1.0, -0.4), loc='lower right', borderaxespad=0.)\n    \n    if show_pred_mask and pred_mask is not None:\n        plt.subplot(1, 3, i)\n        plt.imshow(img, cmap='bone')\n        plt.title('Image with predicted mask')\n        plt.imshow(pred_mask[..., 0], cmap=CMAP1)\n        plt.imshow(pred_mask[..., 1], cmap=CMAP2)\n        plt.imshow(pred_mask[..., 2], cmap=CMAP3)\n        \n        handles = [\n            Rectangle((0, 0), 1, 1, color=CMAP1(1.0)),\n            Rectangle((0, 0), 1, 1, color=CMAP2(1.0)),\n            Rectangle((0, 0), 1, 1, color=CMAP3(1.0))\n        ]\n        labels = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n        plt.axis('off')\n        plt.legend(handles, labels, bbox_to_anchor=(1.0, -0.4), loc='lower right', borderaxespad=0.)\n    \n    \n    plt.tight_layout()\n    plt.show()  ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_image('case131_day0_slice_0066', data)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_image('case131_day0_slice_0066', data, apply_CLAHE=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # testing pred_mask display using true mask\n# display_image('case131_day0_slice_0066', data, pred_mask=get_mask('case131_day0_slice_0066', data),\n#               show_pred_mask=True, apply_CLAHE=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# example image with only stomach segment\ndisplay_image('case123_day20_slice_0065', data, apply_CLAHE=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# example image without any segment\ndisplay_image('case123_day20_slice_0001', data, apply_CLAHE=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.592Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see that the max value in the raw image for the 3 images shown previously are significantly different : 605, 13452, 2546","metadata":{}},{"cell_type":"markdown","source":"## display_multiple_slices","metadata":{}},{"cell_type":"code","source":"def display_multiple_slices(id_array, data, apply_CLAHE=False,\n                            show_pred_mask=False, pred_mask_array=None):\n\n    '''\n    id_array : an array of ids like case123_day20_slice_0001\n    data     : dataframe containing all the image metadata \n    apply_CLAHE : whether or not to apply CLAHE\n    show_pred_mask : if this parameter is False, then true mask will be shown\n                     if it is True, masks from pred_mask_array will be shown\n    pred_mask_array : array of prediction masks\n    '''\n\n    l = len(id_array)\n    rows = np.ceil(l/5).astype(int)\n    max_cols = 5\n    data_subset = data.loc[data.id.isin(id_array), ]\n\n    plt.figure(figsize=(max_cols*3, rows*3))\n\n    for i in range(l):\n        \n        img = cv2.imread(data_subset.image_path.iloc[3*i], cv2.IMREAD_UNCHANGED).astype('float32')  # 3*i because every 3 image paths are same \n                                                                                                    #     and we read 1st entry only\n        mx = np.max(img)\n        if mx > 0:\n            img /= mx\n\n        if apply_CLAHE:\n            clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(2,2))\n            img = (img * 255).astype(np.uint8)\n            img = clahe3.apply(img)\n\n        id_ = data_subset.id.iloc[3*i]\n\n        if show_pred_mask and pred_mask_array is not None:\n            mask = pred_mask_array[3*i]\n        else:\n            mask = get_mask(id_, data)\n\n        plt.subplot(rows, max_cols, i+1)\n        plt.imshow(img, cmap='bone')\n        plt.title(id_)\n        plt.imshow(mask[..., 0], cmap=CMAP1)\n        plt.imshow(mask[..., 1], cmap=CMAP2)\n        plt.imshow(mask[..., 2], cmap=CMAP3)\n        plt.axis('off')\n\n        if i == 0:\n            handles = [\n                Rectangle((0, 0), 1, 1, color=CMAP1(1.0)),\n                Rectangle((0, 0), 1, 1, color=CMAP2(1.0)),\n                Rectangle((0, 0), 1, 1, color=CMAP3(1.0))\n            ]\n            labels = ['Large Bowel', 'Small Bowel', 'Stomach']\n        \n            plt.legend(handles, labels, bbox_to_anchor=(0.0, 1.5), loc='upper left', borderaxespad=0.)\n    \n    plt.tight_layout()\n    plt.show()  ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# using the below data to visualize change in segmentation mask across different slices\n# data.query(\"case == 123 and day == 20 and slice >= 63 and slice <= 70\")\ndisplay_multiple_slices(data.query(\"case == 123 and day == 20 and slice >= 63 and slice <= 70\").id.unique(), \n                        data, apply_CLAHE=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# using the below data to visualize change in segmentation mask across different slices - for a case where all masks are present\n# data.query(\"case == 131 and day == 0 and slice > 60 and slice <= 70\")\ndisplay_multiple_slices(data.query(\"case == 131 and day == 0 and slice > 55 and slice <= 70\").id.unique(), \n                        data, apply_CLAHE=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.593Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"data.loc[data.segmentation.isna(), :]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# How many missing values\ndata.isna().sum()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.594Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Only segmentation column contains NaN / missing values","metadata":{}},{"cell_type":"code","source":"print(f'Num cases : {len(data.case.unique())} \\\n        Num unique days : {len(data.day.unique())}   \\\n        Num unique slices : {len(data.slice.unique())}')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# proportion of different slice sizes\ncount_df = data[['id', 'slice_w', 'slice_h']].drop_duplicates()[['slice_w', 'slice_h']].value_counts().reset_index(name='count')\ncount_df['percent'] = count_df['count']*100 / sum(count_df['count'])\nprint(sum(count_df['count']))\ncount_df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# proportion of different pixel sizes\ncount_df = data[['id', 'px_w', 'px_h']].drop_duplicates()[['px_w', 'px_h']].value_counts().reset_index(name='count')\ncount_df['percent'] = count_df['count']*100 / sum(count_df['count'])\nprint(sum(count_df['count']))\ncount_df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.596Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In both of the 2 cells above, after dropping duplicates, there will be only single entry corresponding to a specific image path.\nHence the sum of counts is equal to the path_df length (or 1/3 of data length).\n\nWe see that all the slices (except 360x310) and pixels are squares (same width and height).\n266x266 is the most frequent slice size - 67%, followed by 360x310 - 29%. \n\nWe could resize to 256x256 or 288x288 image size for training the model (UNet model expects input sizes in multiples of 32), and monitor the performance in 360x310 sliced images to verify if slightly larger size combined with different width-height causes issues","metadata":{}},{"cell_type":"code","source":"day_dist = data[['case', 'day']].drop_duplicates()['case'].value_counts().reset_index(name='num_days')\n\ndisplay(day_dist)\n\nsns.histplot(data=day_dist, x='num_days', bins=range(1, day_dist['num_days'].max() + 1), discrete=True)\nplt.xlabel('Number of Days per Case')\nplt.ylabel('Number of Cases')\nplt.title('Distribution of Days per Case')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.596Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"85 unique cases with most cases having 3 days data","metadata":{}},{"cell_type":"code","source":"slice_dist = data[['case', 'day', 'slice']].drop_duplicates()[['case', 'day']].value_counts().reset_index(name='num_slices')\ndisplay(slice_dist)\n\nsns.histplot(data=slice_dist, x='num_slices', bins=range(1, slice_dist['num_slices'].max() + 1), discrete=True)\nplt.xlabel('Number of slices per case-days')\nplt.ylabel('Number of specific case-days')\nplt.title('Distribution of slices per case-day')\nplt.show()\n\ndisplay(slice_dist.num_slices.value_counts())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.597Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"274 unique case-days having mostly 144 slices and few of them with 80 slices","metadata":{}},{"cell_type":"code","source":"slice_dist.loc[slice_dist.num_slices == 80, :]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"case_day_slice_df = data[['case', 'day', 'slice', 'slice_w', 'slice_h']].drop_duplicates()\ncase_day_slice_df.merge(case_day_slice_df, on=['case', 'day']).query(\"(slice_w_x != slice_w_y) | (slice_h_x != slice_h_y)\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"All slices within a specific case-day have the same slice_w and slice_h","metadata":{}},{"cell_type":"code","source":"case_day_slice_df = data[['case', 'day', 'slice', 'slice_w', 'slice_h']].drop_duplicates()\ncase_day_slice_df.merge(case_day_slice_df, on=['case']).query(\"(slice_w_x != slice_w_y) | (slice_h_x != slice_h_y)\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"But within a specific case multiple days can have different slice_w and slice_h","metadata":{}},{"cell_type":"code","source":"case_day_slice_df = data[['case', 'day', 'slice', 'px_w', 'px_h']].drop_duplicates()\ncase_day_slice_df.merge(case_day_slice_df, on=['case', 'day']).query(\"(px_w_x != px_w_y) | (px_h_x != px_h_y)\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"case_day_slice_df = data[['case', 'day', 'slice', 'px_w', 'px_h']].drop_duplicates()\ncase_day_slice_df.merge(case_day_slice_df, on=['case']).query(\"(px_w_x != px_w_y) | (px_h_x != px_h_y)\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Same for pixel sizes. All slices within specific case-day have same pixel size, but different days within same case can have different pixel size.","metadata":{}},{"cell_type":"markdown","source":"## EDA - missing masks","metadata":{}},{"cell_type":"code","source":"num_missing_seg_masks = data.segmentation.isna().sum() \nprint(f'Missing Seg Mask \\n count = {num_missing_seg_masks}\\n percentage = {num_missing_seg_masks/len(data)*100}')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Approximately 70% of entries have missing segmentation masks","metadata":{}},{"cell_type":"code","source":"data['class'].value_counts()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.600Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As expected (previously seen that number of rows in data df is 3x that of path_df), all 3 segmentation classes have equal number of entries.","metadata":{}},{"cell_type":"code","source":"na_counts = (\n    data.groupby('class')['segmentation']\n    .apply(lambda s: s.isna().sum())\n    .reset_index(name='count')\n)\nna_counts['percent'] = 100 * na_counts['count'] / data.groupby('class')['segmentation'].size().values\n\ndisplay(na_counts)\n\nsns.set_style(\"whitegrid\") \nax = sns.barplot(data=na_counts, x='class', y='percent', palette=[CMAP1(1.0), CMAP2(1.0), CMAP3(1.0)])\n\nfor i, row in na_counts.iterrows():\n    ax.text(i, row['percent'] + 1,  # position just above the bar\n            f\"{row['percent']:.2f}% ({row['count']})\",\n            ha='center', va='bottom', fontsize=10)\n\nplt.ylabel('Percentage')\nplt.xlabel('Segmentation Class')\nplt.title('Missing Segmentation Masks')\nplt.yticks(range(0, 105, 10))\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"case_day_seg_missing = (\n     data[['case', 'day', 'class', 'segmentation']]\n     .groupby(['case', 'day', 'class'])['segmentation']\n     .apply(lambda s: s.isna().sum())\n     .reset_index(name='count').sort_values(by='count', ascending=False)\n)\ndisplay(case_day_seg_missing)\n\n\n# sns.set_style(\"whitegrid\") \n# ax = sns.barplot(data=na_counts, x='class', y='percent', palette=[CMAP1(1.0), CMAP2(1.0), CMAP3(1.0)])\n\n# for i, row in na_counts.iterrows():\n#     ax.text(i, row['percent'] + 1,  # position just above the bar\n#             f\"{row['percent']:.2f}% ({row['count']})\",\n#             ha='center', va='bottom', fontsize=10)\n\n# plt.ylabel('Percentage')\n# plt.xlabel('Segmentation Class')\n# plt.title('Missing Segmentation Masks')\n# plt.yticks(range(0, 105, 10))\n# plt.show()\n\nsns.boxplot(data=case_day_seg_missing, x='class', y='count', palette=[CMAP1(1.0), CMAP2(1.0), CMAP3(1.0)])\nsns.stripplot(data=case_day_seg_missing, x='class', y='count', color='black', size=3, jitter=True, alpha=0.4)\nplt.ylabel('Missing Mask Count')\nplt.xlabel('Segmentation Class')\nplt.title('Distribution of Missing Masks per Class (by Case-Day)')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.601Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Only 1 case-day seems to have all 144 slices missing large_bowel (case 43 - day 26). Lets visualize those slices.","metadata":{}},{"cell_type":"code","source":"# display_multiple_slices(data.query(\"case == 43 and day == 26\").id.unique(), \n#                         data, apply_CLAHE=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#visualizing the original image and image with true mask for border slices where segmentation classes just start appearing/disapperaing\ndisplay_image('case43_day26_slice_0057', data, apply_CLAHE=True)\ndisplay_image('case43_day26_slice_0058', data, apply_CLAHE=True)\ndisplay_image('case43_day26_slice_0121', data, apply_CLAHE=True)\ndisplay_image('case43_day26_slice_0122', data, apply_CLAHE=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"day 15 for case 117 has the least number of missing. Lets visualize that too","metadata":{}},{"cell_type":"code","source":"# display_multiple_slices(data.query(\"case == 117 and day == 15\").id.unique(), \n#                         data, apply_CLAHE=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#visualizing the original image and image with true mask for border slices where segmentation classes just start appearing/disapperaing\ndisplay_image('case117_day15_slice_0009', data, apply_CLAHE=True)\ndisplay_image('case117_day15_slice_0010', data, apply_CLAHE=True)\ndisplay_image('case117_day15_slice_0065', data, apply_CLAHE=True)\ndisplay_image('case117_day15_slice_0066', data, apply_CLAHE=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.603Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The general structure of all slices per day seem to be that only the middle slices have the segmentation classes visible.","metadata":{}},{"cell_type":"markdown","source":"# Train validation split\n\nThe competition data description mentions that there are some cases with early days in train and later days in test, and some other cases where entirety of case is in train or test.\n\nWe could split cases into 2 - set1, set2 where set1 could be used for partially unseen cases and set2 for wholly unseen cases.\nset1 could be created as  : for each case 80% of early days in train, and 20% later days in test\nset2 could be created as  : 80% of cases in train and 20% in test\n\nAlso, in both these approaches it would be good to incorporate empty segmentation mask percentage.\n\nBut the number of days in different cases is different, and as we have seen previously, there are large number of cases with 1 or 2 or 3 days. This would make set1 based approach more complicated. So for now, we'll rely on only set2 based approach.","metadata":{}},{"cell_type":"code","source":"sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\nindex_train, index_valid = next(sgkf.split(data.id, data.segmentation.isna(), data.case))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(index_train), len(index_valid)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train = data.iloc[index_train, :]\ndata_valid = data.iloc[index_valid, :]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_valid","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.605Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Using data subset\nTo start off with, we'll use a smaller dataset by considering around 1/10 th of the cases","metadata":{}},{"cell_type":"code","source":"print(len(data_train.case.unique()), len(data_valid.case.unique()))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train_sub = data_train.loc[data_train.case.isin(data_train.case.unique()[:11]), :]\ndata_valid_sub = data_valid.loc[data_valid.case.isin(data_valid.case.unique()[:2]), :]\n\nprint(len(data_train_sub), len(data_valid_sub), len(data_train_sub)/len(data_valid_sub))\n\n# Note : 11, 2 numbers obtained by manually trying numbers \n#              with 18/10 ~ 2 for valid and such that train len/valid len ~ 4 similar to 4 splits for train and 1 split for valid","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_masks_train = data_train_sub.segmentation.isna().sum() \nmissing_masks_valid = data_valid_sub.segmentation.isna().sum() \nprint(missing_masks_train, missing_masks_train*100/len(data_train_sub))\nprint(missing_masks_valid, missing_masks_valid*100/len(data_valid_sub))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.606Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"approximately similar","metadata":{}},{"cell_type":"code","source":"na_counts_train = (\n    data_train_sub.groupby('class')['segmentation']\n    .apply(lambda s: s.isna().sum())\n    .reset_index(name='count')\n)\nna_counts_train['percent'] = 100 * na_counts_train['count'] / data_train_sub.groupby('class')['segmentation'].size().values\n\ndisplay(na_counts_train)\n\n\nna_counts_valid = (\n    data_valid_sub.groupby('class')['segmentation']\n    .apply(lambda s: s.isna().sum())\n    .reset_index(name='count')\n)\nna_counts_valid['percent'] = 100 * na_counts_valid['count'] / data_valid_sub.groupby('class')['segmentation'].size().values\n\ndisplay(na_counts_valid)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.607Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"roughly same with some difference in large_bowel missing percentage","metadata":{}},{"cell_type":"code","source":"data_train_sub = data_train_sub.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_valid_sub = data_valid_sub.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.608Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# intensity ranges of images are inconsistent as seen in this cell's output shown commented\n# so it is better to use per image scaling while loading images instead of globally normalizing by 65535(since image is 16 bit)\n\n# def read_image(id_, data):\n#   data_subset = data.loc[data.id == id_]\n#   img = cv2.imread(data_subset.image_path.iloc[0], cv2.IMREAD_UNCHANGED)\n#   return img\n\n# sample_image = read_image('case123_day0_slice_0001', data)\n# print(np.min(sample_image), np.max(sample_image))\n\n# sample_image = read_image('case123_day0_slice_0002', data)\n# print(np.min(sample_image), np.max(sample_image))\n\n# sample_image = read_image('case123_day0_slice_0003', data)\n# print(np.min(sample_image), np.max(sample_image))\n\n# sample_image = read_image('case123_day20_slice_0001', data)\n# print(np.min(sample_image), np.max(sample_image))\n\n# sample_image = read_image('case123_day22_slice_0001', data)\n# print(np.min(sample_image), np.max(sample_image))\n\n# sample_image = read_image('case42_day0_slice_0001', data)\n# print(np.min(sample_image), np.max(sample_image))\n\n# sample_image = read_image('case42_day17_slice_0001', data)\n# print(np.min(sample_image), np.max(sample_image))\n\n# sample_image = read_image('case42_day19_slice_0001', data)\n# print(np.min(sample_image), np.max(sample_image))\n\n# sample_image = read_image('case129_day0_slice_0001', data)\n# print(np.min(sample_image), np.max(sample_image))\n\n# sample_image = read_image('case129_day20_slice_0001', data)\n# print(np.min(sample_image), np.max(sample_image))\n\n# sample_image = read_image('case129_day22_slice_0001', data)\n# print(np.min(sample_image), np.max(sample_image))\n\n# # 0 3621\n# # 0 3553\n# # 0 2822\n# # 0 2546\n# # 0 6886\n# # 0 2322\n# # 0 1332\n# # 0 4326\n# # 0 521\n# # 0 363\n# # 0 200","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GITractDataset(Dataset):\n    def __init__(self, df, is_train=True, transforms=None):\n        self.df = df\n        self.id_ = df['id'].unique()\n        self.is_train = is_train\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.id_)\n    \n    def __getitem__(self, idx):\n        id_ = self.id_[idx]\n        img = load_image(id_, self.df)\n        img = np.tile(img[..., None], [1, 1, 3])\n        if self.is_train:\n            mask = get_mask(id_, self.df)\n            if self.transforms:\n                augmented = self.transforms(image=img, mask=mask)\n                img = augmented['image']\n                mask = augmented['mask']\n            return img, mask, id_\n        else:\n            if self.transforms:\n                augmented = self.transforms(image=img)\n                img = augmented['image']\n                data_sub = self.df.loc[self.df.id == id_]\n                height = data_sub.slice_h.iloc[0]\n                width = data_sub.slice_w.iloc[0]\n            return img, id_, height, width","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.609Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"transform_train = A.Compose([\n    A.Resize(IMAGE_RESIZE[0], IMAGE_RESIZE[1], interpolation=cv2.INTER_NEAREST,\n             mask_interpolation=cv2.INTER_NEAREST,),\n    A.Normalize(mean=IMAGE_NORMALIZE_MEAN, std=IMAGE_NORMALIZE_SD, max_pixel_value=1.0),\n    A.ToTensorV2(transpose_mask = True),\n])\n\ntransform_valid = A.Compose([\n    A.Resize(IMAGE_RESIZE[0], IMAGE_RESIZE[1], interpolation=cv2.INTER_NEAREST,\n             mask_interpolation=cv2.INTER_NEAREST,),\n    A.Normalize(mean=IMAGE_NORMALIZE_MEAN, std=IMAGE_NORMALIZE_SD, max_pixel_value=1.0),\n    A.ToTensorV2(transpose_mask = True),\n])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_train = GITractDataset(data_train_sub, transforms=transform_train)\ndataset_valid = GITractDataset(data_valid_sub, transforms=transform_valid)\n\ndataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE_TRAIN, shuffle=True, num_workers=DATA_LOADER_NUM_WORKERS)\ndataloader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE_VALID, shuffle=False, num_workers=DATA_LOADER_NUM_WORKERS)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = next(iter(dataloader_train))\nimg, mask, id_ = dataset\nprint(img.shape, mask.shape, len(id_))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = 31\nnp.max(img[idx].numpy()), np.min(img[idx].numpy())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(img[idx].numpy()[0, 0, 0]), type(mask[idx].numpy()[0, 0, 0])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_dataset(dataset, display_orig=False, num_images=None, denormalize=False, apply_CLAHE=False):\n\t'''\n\tdataset : dataset to be displayed\n\tdisplay_orig : Should the original images prior to augmentation be shown alongside images after augmentation.\n\t\t\t\t   In this case 1st 5 images before and after augmentation is shown and num_images parameter value is ignored\n\tnum_images : Number of images to be shown. Defaults to the full dataset size i.e. the batch size\n\tdenormalize : Set to True if A.normalize has been applied as part of augmentations and you wish to denormalize it\n\t'''\n\timg_arr, mask_arr, id_arr = dataset\n\tif num_images is None:\n\t\tnum_images = len(img_arr)\n\tmax_cols = 5\n\t\n\tif display_orig:\n\t\tnum_images = 5\n\t\trows = 2\n\t\tplt.figure(figsize=(max_cols*3, rows*3))\n\t\tids_shown = list()\n\telse:\n\t\trows = np.ceil(num_images/max_cols).astype(int)\n\t\tplt.figure(figsize=(max_cols*3, rows*3))\n\t\n\tfor idx in range(num_images):\n\t\timg, mask, id_ = img_arr[idx], mask_arr[idx], id_arr[idx]\n\t\timg = img.permute(1,2,0)    #after the permute, img is in HxWxC format\n\t\tif denormalize:\n\t\t\t#print('denormalizing')\n\t\t\t#print(img.shape)\n\t\t\timg = img * torch.tensor(IMAGE_NORMALIZE_SD) + torch.tensor(IMAGE_NORMALIZE_MEAN)\n\t\t\timg = img.clamp(0, 1)\n\t\timg = img.cpu().numpy()\n\t\timg = (img * 255).astype(np.uint8)\n\n\t\tif apply_CLAHE:\n\t\t\tclahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(2,2))\n\t\t\tfor ch in range(3):\n\t\t\t\timg[:,:,ch] = clahe.apply(img[:,:,ch])\n\n\t\tmask = mask.permute(1,2,0).cpu().numpy()\n\n\t\tplt.subplot(rows, max_cols, idx+1)\n\t\t#print(f'just before imshow {id_} min : {np.min(img)} max : {np.max(img)}')\n\t\tprint(img.shape)\n\t\t#print(f'just before imshow mask {id_} min : {np.min(mask)} max : {np.max(mask)}')\n\t\tplt.imshow(img[:, :, 0], cmap='bone')  #img was tiled grayscale, to display just use any 1 channel\n\t\tplt.title(f'{idx} : {id_}')\n\t\t\n\t\tplt.imshow(mask[..., 0], cmap=CMAP1)\n\t\tplt.imshow(mask[..., 1], cmap=CMAP2)\n\t\tplt.imshow(mask[..., 2], cmap=CMAP3)\n\t\tplt.axis('off')\n\n\t\tif idx == 0:\n\t\t\thandles = [\n\t\t\t\tRectangle((0, 0), 1, 1, color=CMAP1(1.0)),\n\t\t\t\tRectangle((0, 0), 1, 1, color=CMAP2(1.0)),\n\t\t\t\tRectangle((0, 0), 1, 1, color=CMAP3(1.0))\n\t\t\t]\n\t\t\tlabels = ['Large Bowel', 'Small Bowel', 'Stomach']\n\t\t\tplt.legend(handles, labels, bbox_to_anchor=(0.0, 1.5), loc='upper left', borderaxespad=0.)\n\n\t\tif display_orig:\n\t\t\tids_shown.append(id_)\n\n\tif display_orig:\n\t\tprint(ids_shown)\n\n\t\tfor id_ in ids_shown:\n\t\t\tidx += 1\n\t\t\timg = load_image(id_, data)\n\t\t\timg = (img * 255).astype(np.uint8) # 0-255 range required for CLAHE. \n\t\t\t\t\t\t\t\t\t\t\t   # Using this in general to maintain consistency with the case where CLAHE is required\n\t\t\tif apply_CLAHE:\n\t\t\t\tclahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(2,2))\n\t\t\t\timg = clahe.apply(img)\n\t\t\t\t\n\t\t\tmask = get_mask(id_, data)\n\t\t\t\n\t\t\tplt.subplot(rows, max_cols, idx+1)\n\t\t\t#print(f'just before imshow orig {id_} min : {np.min(img)} max : {np.max(img)}')\n\t\t\tprint(img.shape)\n\t\t\tplt.imshow(img, cmap='bone')\n\t\t\tplt.title('Original')\n\t\t\tplt.imshow(mask[..., 0], cmap=CMAP1)\n\t\t\tplt.imshow(mask[..., 1], cmap=CMAP2)\n\t\t\tplt.imshow(mask[..., 2], cmap=CMAP3)\n\t\t\tplt.axis('off')\n\n\tplt.tight_layout()\n\tplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_dataset(dataset, num_images=5, denormalize=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_dataset(dataset, display_orig=True, denormalize=True, apply_CLAHE=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.613Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# https://smp.readthedocs.io/en/latest/encoders_timm.html\nsmp_encoder_weights = None if TEST_PREDICT and LOAD_MODEL_FOR_TEST_PREDICT else 'imagenet'\nmodel = smp.Unet(\n    encoder_name = 'efficientnet-b1',        \n    encoder_weights = smp_encoder_weights,     \n    in_channels = 3,                  \n    classes = NUM_CLASSES,                      \n)\nmodel.to(DEVICE)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optimizer","metadata":{}},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=1e-3)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss function","metadata":{}},{"cell_type":"code","source":"dice_loss = smp.losses.DiceLoss(mode='multilabel') \nBCE_loss = smp.losses.SoftBCEWithLogitsLoss()\n\ndef loss_fn(y_pred, y_true, loss_wt = 0.5):\n    return dice_loss(y_pred, y_true) * loss_wt + BCE_loss(y_pred, y_true) * (1 - loss_wt)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.615Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"markdown","source":"## Dice Metric class","metadata":{}},{"cell_type":"code","source":"class DiceScoreCustom:\n\tdef __init__(self, num_classes, eps=1e-6):\n\t\tself.num_classes = num_classes\n\t\tself.eps = eps\n\t\tself.reset()\n\n\tdef reset(self):\n\t\tself.dice_sum = 0.0\n\t\tself.image_count = 0\n\n\t\t# per-organ totals\n\t\tself.organ_dice_sum = torch.zeros(self.num_classes)\n\t\tself.organ_count = torch.zeros(self.num_classes)\n\n\tdef update(self, preds, targets):\n\t\t\"\"\"\n\t\tpreds, targets: (B, C, H, W) binary {0,1} tensors\n\t\tImplements host comment: skip organs where both pred & target are empty : https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/324934\n\t\t\"\"\"\n\t\tI = (targets & preds).sum((2, 3))\n\t\tU = (targets | preds).sum((2, 3))\n\n\t\t# Dice per organ (B, C)\n\t\tdice = (2 * I) / (U + I + self.eps)\n\n\t\t# Mask out empty organs (where both pred and gt are 0)\n\t\tnon_empty = U > 0  # [B, C]\n\t\t#dice = dice * non_empty   # not needed, if empty then dice is already 0\n\n\t\t# For each image (B), compute mean over valid organs only\n\t\torgan_counts = non_empty.sum(dim=1)  # [B]\n\t\tdice_per_image = dice.sum(dim=1) / organ_counts.clamp(min=1)\n\n\t\t#note : accumulators below are moved to CPU to reduce GPU memory usage\n\n\t\t# accumulate global\n\t\tself.dice_sum += dice_per_image.sum().item()\n\t\tself.image_count += dice_per_image.numel()\n\n\t\t# accumulate per-organ\n\t\tself.organ_dice_sum += dice.sum(dim=0).detach().cpu()\n\t\tself.organ_count += non_empty.sum(dim=0).detach().cpu()\n\n\tdef compute(self):\n\t\t\"\"\"\n\t\tReturns:\n\t\t\toverall dice: scalar tensor\n\t\t\tper_organ dice: (C,) tensor\n\t\t\"\"\"\n\t\toverall = (\n\t\t\ttorch.tensor(self.dice_sum / self.image_count)\n\t\t\tif self.image_count > 0 \n\t\t\telse torch.tensor(0.0)\n\t\t)\n\t\tper_organ = torch.where(\n\t\t\tself.organ_count > 0,\n\t\t\tself.organ_dice_sum / self.organ_count,\n\t\t\ttorch.tensor(0.0)\n\t\t)\n\t\treturn overall, per_organ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.615Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hausdorff distance class","metadata":{}},{"cell_type":"code","source":"class HausdorffDistanceCustom:\n\tdef __init__(self, num_classes):\n\t\tself.num_classes = num_classes\n\t\tself.reset()\n\n\tdef reset(self):\n\t\tself.h3d_sum = 0.0\n\t\tself.image3d_count = 0\n\n\t\tself.organ_h3d_sum = np.zeros(self.num_classes)\n\t\tself.organ_count_sum = np.zeros(self.num_classes)\n\n\tdef _compute_hausdorff_per_organ(self, preds, targets):\n\t\t'''\n\t\tpreds and targets : (Depth, Height, Width) binary {0,1} tensors\n\t\t'''\n\t\tif np.all(preds == targets):\n\t\t\treturn 0.0\n\t\n\t\t(edges_preds, edges_targets) = get_mask_edges(preds, targets)\n\t\tsurface_distance = get_surface_distance(edges_preds, edges_targets, distance_metric=\"euclidean\")\n\t\n\t\tif surface_distance.shape == (0,):\n\t\t\treturn 0.0\n\t\tdist = surface_distance.max()\n\t\tmax_dist = np.sqrt(np.sum((np.array(preds.shape) - 1) ** 2))\n\t\n\t\tif dist > max_dist:\n\t\t\treturn 1.0\n\t\n\t\treturn dist / max_dist\n\n\tdef update(self, preds, targets):\n\t\t'''\n\t\tpreds and targets : (Channel, Depth, Height, Width) binary {0,1} tensors\n\t\t'''\n\n\t\tU = (targets | preds).sum((1, 2, 3))  # [C]\n\n\t\thausdorff = np.array([self._compute_hausdorff_per_organ(preds[i, ...], targets[i, ...]) for i in range(NUM_CLASSES)])  # [C]\n\n\t\t# Mask out empty organs (where both pred and gt are 0)\n\t\tnon_empty = U > 0  # [C]\n\n\t\torgan_count = non_empty.sum()\n\n\t\tif organ_count != 0:\n\t\t\thausdorff_per_3dimage = hausdorff.sum() / organ_count\n\n\t\t\t# accumulate global\n\t\t\tself.h3d_sum += hausdorff_per_3dimage\n\t\t\tself.image3d_count += 1\n\n\t\t# accumulate per-organ\n\t\tself.organ_h3d_sum += hausdorff\n\t\tself.organ_count_sum += non_empty\n\n\tdef compute(self):\n\t\t\"\"\"\n\t\tReturns:\n\t\t\toverall hausdorff: scalar\n\t\t\tper_organ hausdorff: (C,)\n\t\t\"\"\"\n\t\toverall = self.h3d_sum / self.image3d_count\n\n\t\tper_organ = self.organ_h3d_sum / self.organ_count_sum\n\n\t\treturn overall, per_organ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dice_score_obj = DiceScoreCustom(num_classes=NUM_CLASSES)\nhausdorff_obj = HausdorffDistanceCustom(num_classes=NUM_CLASSES)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.616Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def one_epoch_train(epoch):\n    \n    model.train() #set model in training mode\n    running_loss = 0.0\n    \n    loop = tqdm(dataloader_train, desc=f'Epoch {epoch+1}/{EPOCHS}')\n    for data in loop:\n        imgs, masks, ids = data\n        imgs, masks = imgs.to(DEVICE, dtype=torch.float), masks.to(DEVICE, dtype=torch.float)\n        optimizer.zero_grad()\n        pred_masks = model(imgs)\n\n        loss = loss_fn(pred_masks, masks)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n\n    avg_loss = running_loss / len(dataloader_train)\n    return avg_loss","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"slices80_casedays = set(\n    data_valid[['case', 'day', 'slice']]\n    .drop_duplicates()\n    .value_counts(['case', 'day'])\n    .loc[lambda s: s == 80]\n    .index\n)\n#slices80_casedays","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def one_epoch_valid():\n\n    model.eval()\n    with torch.no_grad():\n        running_loss = 0.0\n        pred_masks_dict, masks_dict = {}, {}\n        for data in dataloader_valid:\n            imgs, masks, ids = data\n            imgs, masks = imgs.to(DEVICE, dtype=torch.float), masks.to(DEVICE, dtype=torch.float)\n            pred_masks = model(imgs)\n            loss = loss_fn(pred_masks, masks)\n            running_loss += loss.item()\n\n            pred_masks = (torch.sigmoid(pred_masks) > 0.5).int()\n            masks = masks.int()\n            dice_score_obj.update(pred_masks, masks)\n\n            #loop through predictions and true masks to create 3D volume with all slices per caseday for Hausdorff computation\n            for p, m, id_ in zip(pred_masks, masks, ids):\n                match = re.match(r\"case(\\d+)_day(\\d+)_slice_(\\d+)\", id_)\n                if match:\n                    caseid, dayid, sliceid = map(int, match.groups())\n\n                casedayid = (caseid, dayid) \n\n                pred_masks_dict.setdefault(casedayid, []).append((sliceid, p))\n                masks_dict.setdefault(casedayid, []).append((sliceid, m))\n\n                #in the data, casedays have either 144 slices or 80 slices\n                if (len(pred_masks_dict[casedayid]) == 144) or (casedayid in slices80_casedays and len(pred_masks_dict[casedayid]) == 80):\n                    pred_masks_sorted = [p.cpu().numpy() for sid, p in sorted(pred_masks_dict[casedayid], key=lambda x: x[0])]\n                    masks_sorted = [m.cpu().numpy() for sid, m in sorted(masks_dict[casedayid], key=lambda x: x[0])]\n\n                    pred_masks_volume = np.stack(pred_masks_sorted, axis=1)\n                    masks_volume = np.stack(masks_sorted, axis=1)\n\n                    hausdorff_obj.update(pred_masks_volume, masks_volume)\n\n                    #free memory\n                    del pred_masks_dict[casedayid], masks_dict[casedayid]\n\n            \n        avg_loss = running_loss / len(dataloader_valid)\n\n        epoch_dice_score = dice_score_obj.compute()\n        dice_score_obj.reset()\n        \n        epoch_hausdorff = hausdorff_obj.compute()\n        hausdorff_obj.reset()    \n        \n    return avg_loss, epoch_dice_score, epoch_hausdorff\n    ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if TRAIN_VALID_SPLIT:\n    for epoch in range(EPOCHS):\n        loss_train = one_epoch_train(epoch)\n        loss_valid, dice_score, hausdorff = one_epoch_valid()\n        dice_overall, dice_per_organ = dice_score\n        hausdorff_overall, hausdorff_per_organ = hausdorff\n        combined_metric = 0.4*dice_overall + 0.6*(1-hausdorff_overall)\n        print(\n            f'Epoch {epoch+1} | '\n            f'Train Loss: {loss_train:.3f} | Valid Loss: {loss_valid:.3f} | '\n            f'Combined metric: {combined_metric:.3f} | '\n            f'Dice: {dice_overall:.3f} (LB {dice_per_organ[0]:.3f}, SB {dice_per_organ[1]:.3f}, S {dice_per_organ[2]:.3f}) | '\n            f'Hausdorff: {hausdorff_overall:.3f} (LB {hausdorff_per_organ[0]:.3f}, SB {hausdorff_per_organ[1]:.3f}, S {hausdorff_per_organ[2]:.3f})'\n        )","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if TRAIN_VALID_SPLIT and SAVE_TRAIN_VALID_MODEL:\n    torch.save(model.state_dict(), MODEL_PARAMS_FILE_NAME)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.619Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict on test data","metadata":{}},{"cell_type":"code","source":"if TEST_PREDICT: \n    if LOAD_MODEL_FOR_TEST_PREDICT:\n        model.load_state_dict(torch.load(MODEL_PARAMS_LOAD_FILE_PATH))\n    model.eval()\n\n    data_test = pd.read_csv(DIR_PATH + \"sample_submission.csv\")\n    test_set_hidden = not bool(len(data_test))\n    if test_set_hidden:\n        data_test = data_valid_sub  # Use validation data for testing the code prior to submission\n    else:\n        data_test[['case', 'day', 'slice']] = data_test['id'].str.extract(r'case(\\d+)_day(\\d+)_slice_(\\d+)')\n        path_df = get_path_df(train = False)\n\n        data_test = data_test.merge(path_df, on = ['case', 'day', 'slice'])\n    \n        int_cols = ['case', 'day', 'slice', 'slice_w', 'slice_h']\n        data_test[int_cols] = data_test[int_cols].astype(np.uint32)\n    \n        float_cols = ['px_w', 'px_h']\n        data_test[float_cols] = data_test[float_cols].astype(np.float32)\n\n    transform_test = A.Compose([\n        A.Resize(IMAGE_RESIZE[0], IMAGE_RESIZE[1], interpolation=cv2.INTER_NEAREST),\n        A.Normalize(mean=IMAGE_NORMALIZE_MEAN, std=IMAGE_NORMALIZE_SD, max_pixel_value=1.0),\n        A.ToTensorV2(transpose_mask = False),\n    ])    \n    dataset_test = GITractDataset(data_test, is_train=False, transforms=transform_test)\n    dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE_TEST, shuffle=False, num_workers=DATA_LOADER_NUM_WORKERS)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if TEST_PREDICT:\n    test_ids, test_class, test_pred_RLE = [], [], []   # data to be written to submission file\n    \n    with torch.no_grad():\n        for imgs, ids, heights, widths in dataloader_test:\n            imgs = imgs.to(DEVICE, dtype=torch.float)\n            pred_masks = model(imgs)\n            pred_masks = (torch.sigmoid(pred_masks) > 0.5).int()\n            pred_masks = pred_masks.permute(0, 2, 3, 1).cpu().numpy()   # shape after permute [B, H, W, C]\n\n            for mask, id_, h, w in zip(pred_masks, ids, heights, widths):\n                mask_orig_size = cv2.resize(mask, dsize=(w.item(), h.item()), interpolation=cv2.INTER_NEAREST)\n                rles = [rle_encode(mask_orig_size[..., chid]) for chid in range(NUM_CLASSES)]\n                \n                test_ids.extend([id_] * NUM_CLASSES)\n                test_class.extend(CLASS_NAMES)\n                test_pred_RLE.extend(rles)\n\n    submission_df = pd.DataFrame({\n        'id': test_ids, \n        'class': test_class, \n        'predicted': test_pred_RLE\n    })\n    submission_df.to_csv('submission.csv', index=False)\n    !head submission.csv\n    display(submission_df.loc[submission_df.predicted != \"\"].head())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-29T11:27:31.620Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# References\n* https://www.kaggle.com/code/awsaf49/uwmgi-mask-data\n* https://www.kaggle.com/code/paulorzp/run-length-encode-and-decode\n* https://www.kaggle.com/code/awsaf49/uwmgi-unet-train-pytorch\n* https://www.kaggle.com/code/andradaolteanu/aw-madison-eda-in-depth-mask-exploration\n* https://www.kaggle.com/code/masatomurakawamm/uwmgi-pspnet-u-net-deeplabv3-swin-unet\n* https://www.kaggle.com/code/clemchris/gi-seg-pytorch-train-infer : for metrics\n* https://www.kaggle.com/code/carnozhao/tract-competiton-metrics/notebook : for metrics\n* https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/324432 : metrics\n* https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/324934 : mentions that \"when mask and pred are 0, not included in metric\"\n* https://www.kaggle.com/code/yiheng/50-times-faster-way-get-hausdorff-with-monai","metadata":{}}]}